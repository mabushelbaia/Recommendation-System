\chapter{Requirements And Literature Review}
\minitoc 

\section{Functional Requirements}

The system should provide a RESTful API as the final interface to be used by the front-end application.
The API provides endpoints that allow inserting customers, products, and interactions. In addition to endpoints for retrieving the recommendations for a given customer.

\section{System Requirements}

In order for the system to be useful it has to meet the following specifications:

\subsection{Scalability}
Scalability implies that it has to be cloud-native, the inference system should apply proper load balancing across multi-node, multi-model deployments.

\subsection{Real-time Predictions}
To be usable in any website or application, the system should be able to provide real-time predictions, and suggestions, with a few milliseconds latency. \\

To fulfill this requirement, trained models should run on optimized inference servers or services, the suggested deployment plan is to use
\textbf{Nvidia Triton}
\footnote{Nvidia Triton Inference Server, part of the Nvidia AI platform and available with Nvidia AI Enterprise, is open-source software that standardizes AI model deployment and execution across every workload \cite{Triton}}. 
inference server \cite{Triton}, 
integrated with \textbf{Amazon SageMaker} model deployment \cite{SageMaker} as infrastructure.

\subsection{Near Real-time Training}

This implies continuous training and deployment of the model which requires the automation of training and deployment.

\subsection{Elasticity And Optimization}

Elasticity is vital for keeping up with traffic spikes and declines while optimizing infrastructure costs. To achieve this, the system should be able to scale up and down based on the traffic and load.

\subsection{Security}

Like any other system, the system has to be immune to security threats by implementing best practices at every level in the deployment and design. \\ \\
For example, rate-limiting requests to interaction injection endpoints, using attestation when possible, and limiting access to user and product create, read, update, and delete (CRUD) operations.
 

\section{Recommendation System Architecture}
\subsection{Recommendation System Stages}
Any system is a group of components that work together to achieve a goal according to a set of rules. The recommendation system is no different, it is a group of components that work together to provide personalized suggestions to users. There are two recommendation system types based on the number of stages they have:
\subsection*{Two-stage Recommender Systems}
Two-stage recommender systems are systems that have two main stages: candidate generation and ranking. The candidate generation stage is responsible for generating a set of candidate items for each user, while the ranking stage is responsible for ranking the candidate items and selecting the top items to be recommended to the user. The candidate generation stage is usually based on collaborative filtering or content-based filtering, while the ranking stage is usually based on machine learning models such as matrix factorization or deep learning models.\cite{MultiStageRecSys}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{assets/Two_stage_rec_sys.jpg}
    \caption{Two-stage Recommender System\cite{MultiStageRecSys}}
\end{figure}
\subsection*{Four-stage Recommender Systems}
Four-stage recommender systems are systems that have four main stages: retrieval, filtering, scoring, and ordering. 
The retrieval stage is responsible for retrieving a set of candidate items for each user, 
the filtering stage is responsible for filtering the candidate items according to business rules and constraints,
and the ordering stage is responsible for ordering the scored items and selecting the top items to be recommended to the user. 
The retrieval stage is usually based on collaborative filtering or content-based filtering, while the filtering stage is 
usually based on machine learning models such as matrix factorization or deep learning models, and the scoring and ordering 
stages are usually based on machine learning models such as deep learning models.\cite{NvidiaRecSysBestPractices}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{assets/Four_stage_rec_sys.png}
    \caption{Four-stage Recommender System\cite{NvidiaRecSysBestPractices}}
\end{figure}
each stage utilizes a set of components and algorithms in training and inference processes to achieve its goal.
\subsection*{Trianing (Offline)}
Like any machine learning system, the process of training the models occurs offline,
where the system is trained on a set of historical data, aka batched data, to optimize the model's parameters.


\subsection*{Real-Time Inference (Online)}
The inference phase is when the system makes the predictions and suggestions for the application, 
in order to be useful in any website or application, 
the system should be able to provide real-time predictions, 
and suggestions, with a few hundred milliseconds of latency.

In online inference, the system computes the recommendation in real-time based on the user's interactions and preferences, usually with user actions being the trigger for the recommendation generation process.

\subsection*{Batch Inference (Offline)}
Unlike online inference, batch inference is the process of generating recommendations in advance and storing them, 
where the system computes the recommendations for all users and items, 
the process is repeated on a periodical basis.\cite{NvidiaOfflineToOnline}

This process allows much shorter response times and is useful for systems with a large number of users and items and a high volume of traffic.

Online and batched inference can be used together to provide real-time recommendations, where the system periodically pre-computes the recommendations for active users, but for new users, the system computes the recommendations in real-time.

