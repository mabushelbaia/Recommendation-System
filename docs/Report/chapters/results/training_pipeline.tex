\section{Expirement Offline Pipeline}

\subsection{Data Preprocessing}

\subsubsection{Converting the Data from CSV to Parquet}

The raw AliCCP dataset is stored in CSV format. However, to leverage GPU-accelerated libraries such as cuDF, the data is converted into the Apache Parquet~\cite{ApacheParquet}.


The original data in CSV format was about 11GB in size. After converting it to Parquet format, the data size was reduced to 1.1GB.
The training data consists of 42,299,771 rows, while the testing data consists of 42,999,775 rows.

\subsubsection{Eliminating Rows with Excessive Missing Data}

Null values can introduce biases or inaccuracies into the analysis, so removing rows with many missing data points helps to mitigate this issue.
Rows with more than five null features were removed to ensure data completeness.
The five nulls threshold ensures that the remaining rows have enough complete information to be useful for training and testing the recommender system.
This resulted in a training dataset with 33,088,764 rows and a testing dataset with 33,678,677 rows.

\subsubsection{Resplitting Data}

Originally the data was split with a 1:1 ratio into training and testing data.
To balance the training and testing data, 60\% of the testing data was moved to the training data. 
The resulting datasets contain 53,299,746 training rows and 13,467,695 testing rows.
The ratio of the training data to the testing data is approximately 80:20.


The following table shows the changes made to the data during the preprocessing phase:

\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Data Set} & \textbf{Original Rows} & \textbf{Data After Cleaning} & \textbf{Data After Resplitting} \\
\hline
Training Data & 42,299,771 & 33,088,764 & 53,299,746 \\
\hline
Testing Data & 42,999,775 & 33,678,677 & 13,467,695 \\
\hline
\end{tabular}
\caption{Data Rows Before and After Preprocessing}
\label{tab:data-reprocessing}
\end{table}

\subsubsection{Saving customer and products parquets}

In this step, the user features as well as the item features are extracted from the training data and saved as parquet files.
Those files will later be used as an offline data store by Feast, the feature store that will be used at inference time.

Before saving the parquet files, timestamps are added to the data to help Feast keep track of the data freshness and do incremental materialization.

\subsection{Retrieval Stage}

As mentioned earlier, the retrieval model consists of two towers, the user tower and the item tower. 
The user tower is responsible for processing the user features, while the item tower is responsible for processing the item features.

\subsubsection{Retrieval Model Training}

The model is trained using the following hyperparameters:

\begin{itemize}
\item Batch Size: 28K
\item Number of Epochs: 30
\item Optimizer: Adam
\item Loss Function: Categorical Crossentropy
\item Model: Two-Tower (v2 Implementation By Merlin Models)
\end{itemize}

Figure \ref{fig: RetrievalModelTraining} shows the training loss and recall of the retrieval model.
It shows that the model achieves a recall at 20 of around 30\% on the training data, but on the validation data, the recall is around 4\% only.

This means that in the top 20 recommendations, 4\% of the actual positive items in the validation set are listed.
In production, not only 20 but the top hundreds of items are taken from the retrieval model and are fed into the ranking model. 

So if the recall is 4\% at 20, it is expected to be much higher at values like 500 which is used in this project.
Unfortunately, due to technical limitations, the recall at 500 could not be calculated for each user of the dataset.

\begin{figure}[H]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/retrieval_trianing_loss.png}
        \caption[Retrieval Model Loss]{Retrieval Model Loss Vs Epochs}
        \label{fig:RetrievalModelLoss}
    \end{subfigure}
    \bigskip
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/retrieval_trianing_recall.png}
        \caption[Retrieval Model Recall]{Retrieval Model Recall At 20 Vs Epochs}
        \label{fig:RetrievalModelRecall}
    \end{subfigure}
    \caption[Retrieval Model Training]{Retrieval Model Training Results}
    \label{fig: RetrievalModelTraining}
\end{figure}

\subsubsection{Saving Query Tower}

Once the retrieval model is trained, the query tower is saved as a Keras model to the model repository, 
in this project, it should be an S3 bucket, but for the simplicity of this experiment, it is saved locally to an EBS GP3 SSD attached to the server.
This model will be used to generate the user embeddings at inference time.

\subsubsection{Generating Item Embeddings}

the embeddings of each item in the dataset are calculated at training time using the trained retrieval model. 
Then those embeddings are saved in a parquet~\cite{ApacheParquet} file which will next be converted to a Faiss~\cite{Faiss} index for fast retrieval at inference time.

Once the retrieval model finishes generating the item embedding parquet,
the parquet is used to generate a Faiss~\cite{Faiss} index.
In this experiment, the index is saved to the model repository and later loaded to GPU memory on inference server start-up.

\subsection{Filtering}
In a four-stage recommender system, the filtering stage typically follows retrieval.
During filtering, products are removed based on business rules, such as stock or age restrictions. 

However, the AliCCP dataset does not include this type of information, so the filtering step was skipped in this experiment.

\subsection{Ranking Model Training}

A DLRM model is trained using the following hyperparameters:

\begin{itemize}
\item Batch Size: 204K
\item Number of Epochs: 2
\item Optimizer: Adam
\item Binary Crossentropy Loss
\item Model: DLRM
\end{itemize}

Figure \ref{fig: RankingModelTraining} shows the training and validation area under the curve of the ranking model vs epochs. It shows that the model starts overfitting after the second epoch, as the training AUC keeps increasing while the validation AUC decreases.
This is why the model was early stopped at two epochs.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{assets/ranking_training_validation_auc.png}
    \caption{Ranking Model Training/Validation Set AUC}
    \label{fig: RankingModelTraining}
\end{figure}

Different models were trained and tested, such as the DCN and NCF, but the DLRM model was chosen as it had slightly better performance and higher metrics.
Figure \ref{fig: RankingModelsResults} shows the metrics of the different models tested.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{assets/Validation Metrics Comparison.png}
    \caption{Ranking Models Validation Metrics Comparison}
    \label{fig: RankingModelsResults}
\end{figure}


Once the model finished training, it was saved as a TensorFlow model to the model repository.