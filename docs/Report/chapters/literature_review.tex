\chapter{Requirements And Literature Review}
\minitoc

\section{Functional Requirements}

The system should provide a RESTful API as the final interface to be used by the front-end application.
The API provides endpoints that allow inserting customers, products, and interactions. In addition to endpoints for retrieving the recommendations for a given customer.

\section{System Requirements}

In order to for the system to be useful it has to meet the following specifications:

\subsection{Scalability}
Scalability implies that it has to be cloud-native, the inference system should apply proper load balancing across multi-node, multi model deployments.

\subsection{Real-time predictions}
To be usable in any website or application, the system should be able to provide real-time predictions, suggestions, with few milliseconds latency. \\

To fulfil this requirement, trained models should run on optimized inference servers or services, the suggested deployment plan is to use
\textbf{Nvidia Triton}
\footnote{Nvidia Triton Inference Server, part of the Nvidia AI platform and available with Nvidia AI Enterprise, is open-source software that standardizes AI model deployment and execution across every workload. \cite{Triton}}
inference server \cite{Triton}, 
integrated with \textbf{Amazon SageMaker} model deployment\cite{SageMaker} as infrastructure.

\subsection{Near Real-time Training}

This implies continuous training and deployment of model which requires the automation of training and deployment.

\subsection{Elasticity And Optimization}

Elasticity is vital for keeping up with traffic spikes and declines while optimizing infrastructure costs. To achieve this, the system should be able to scale up and down based on the traffic and load.

\subsection{Security}

Like any other system, the system has to be immune to security threats by implementing best practices at every level in the deployment and design. \\ \\
\textit{e.g} rate-limiting requests to interaction injection endpoints, using attestation when possible, limiting access to user and product CRUD operations.

\section{Related Work}

There are many open-source and paid solutions that provide recommendation systems and libraries, this section discusses some of them.

\subsection{LightFM \cite{LightFM}}
A Python library that enables the classic matrix factorization techniques to include metadata about both items and users, incorporating both content and collaborative information into the recommendation process ( hybrid ). \\

Its approach is described with more depth in the LightFM paper \cite{kula2015metadata}.

\subsection{Rexy \cite{Rexy}}
Rexy is a Python library that provides a general-purpose recommendation system framework. It is flexible and can be adapted to a variety of data schemas.

\subsection{Gorse \cite{Rexy}}
Gorse is an open source recommender system engine implemented in Go that provides a scalable and flexible recommendation system framework. It supports a variety of algorithms, including collaborative filtering, content-based filtering, and deep learning.

\subsection{AWS Personalize \cite{AWSPersonalize}}
"Amazon Personalize allows developers to quickly build and deploy curated recommendations and intelligent user segmentation at scale using machine learning (ML). Because Amazon Personalize can be tailored to your individual needs, you can deliver the right customer experience at the right time and in the right place." \footnote{AWS description of the service \cite{AWSPersonalize} } \\

\subsection{Google Recommendations AI \cite{GoogleRecommendationsAI}}
Google describes it as "Recommendations AI enables you to build an end-to-end personalized recommendation system based on state-of-the-art deep learning ML models, without a need for expertise in ML or recommendation systems."\footnote{Google Cloud Marketplace \cite{GoogleMarketplaceRecAi}} \\

\subsection{Nvidia Merlin \cite{NvidiaMerlin}}

"NVIDIA Merlin is an open source library providing end-to-end GPU-accelerated recommender systems, from feature engineering and preprocessing to training deep learning models and running inference in production." \footnote{Nvidia Merlin Repository \cite{NvidiaMerlinRepo}} \\

The frameworks, discussed in more depth later, provides many components including:
\begin{itemize}
    \item Merlin Models \cite{MerlinModels}
    \item Merlin NVTabular \cite{MerlinNVTabular}
    \item Merlin HugeCTR \cite{MerlinHugeCTR}
    \item Merlin Transformers4Rec \cite{MerlinTransformer4Rec}
    \item Merlin SOK (SparseOperationsKit)
    \item Merlin Distributed Embeddings (DE)
    \item Merlin Systems \cite{MerlinSystemsRepo}
\end{itemize}

Making it a very customizable and extensible solution.

\begin{table}[h]
    \centering
    \caption{Comparison of Recommendation Solutions}
    \footnotesize
    \setlength{\tabcolsep}{12pt} % Adjust the spacing between columns
    \renewcommand{\arraystretch}{1.5} % Adjust the spacing between rows
    \begin{tabular}{|l|l|}
        \hline
        \textbf{System} & \textbf{LightFM} \\
        \cline{2-2}
        \textbf{License} & Apache 2.0 \\
        \cline{2-2}
        \textbf{Algorithm Type} & Matrix Factorization \\
        \cline{2-2}
        \textbf{Hardware Utilization} & CPU \\
        \cline{2-2}
        \textbf{Deployment Readiness} & Library (Additional Components Needed) \\
        \cline{2-2}
        \textbf{Notes} & - \\
        \hline
        \hline
        \textbf{System} & \textbf{Rexy} \\
        \cline{2-2}
        \textbf{License} & MIT \\
        \cline{2-2}
        \textbf{Algorithm Type} & Matrix Factorization \\
        \cline{2-2}
        \textbf{Hardware Utilization} & CPU \\
        \cline{2-2}
        \textbf{Deployment Readiness} & Library (Additional Components Needed) \\
        \cline{2-2}
        \textbf{Notes} & - \\
        \hline
        \hline
        \textbf{System} & \textbf{Gorse} \\
        \cline{2-2}
        \textbf{License} & Apache 2.0 \\
        \cline{2-2}
        \textbf{Algorithm Type} & Matrix Factorization \\
        \cline{2-2}
        \textbf{Hardware Utilization} & CPU \\
        \cline{2-2}
        \textbf{Deployment Readiness} & Single-node-learning multi-node-inference cluster \\
        \cline{2-2}
        \textbf{Notes} & Unreliable and has many bugs \\
        \hline
        \hline
        \textbf{System} &\textbf{AWS Personalize} \\
        \cline{2-2}
        \textbf{License} & Proprietary \\
        \cline{2-2}
        \textbf{Algorithm Type} & DLRM \\
        \cline{2-2}
        \textbf{Hardware Utilization} & - \\
        \cline{2-2}
        \textbf{Deployment Readiness} & A lot of customization required  \\
        \cline{2-2}
        \textbf{Notes} & High customization, predictions, and training fees \\
        \hline
        \hline
        \textbf{System} & \textbf{Google Recommendations AI} \\
        \cline{2-2}
        \textbf{License} & Proprietary \\
        \cline{2-2}
        \textbf{Algorithm Type} & DLRM \\
        \cline{2-2}
        \textbf{Hardware Utilization} & - \\
        \cline{2-2}
        \textbf{Deployment Readiness} & End-to-End service \\
        \cline{2-2}
        \textbf{Notes} & High predictions, and training fees \\
        \hline
        \hline
        \textbf{System} & \textbf{Nvidia Merlin} \\
        \cline{2-2}
        \textbf{License} & Apache 2.0 \\
        \cline{2-2}
        \textbf{Algorithm Type} & Multiple Options  \\
        \cline{2-2}
        \textbf{Hardware Utilization} & Optimized for Nvidia GPUs \\
        \cline{2-2}
        \textbf{Deployment Readiness} & Recommendation pipelines components \\
        \cline{2-2}
        \textbf{Notes} & Very customizable \\
        \hline
    \end{tabular}
\end{table}
        
