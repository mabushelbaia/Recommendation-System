\chapter{Experiment And Results}
\minitoc

\section{Dataset}

The dataset used in this experiment is called AliCCP and it is a public dataset that contains user-item interactions from an e-commerce platform. The dataset gathers traffic logs of recommendation systems in Taobao, a Chinese online shopping website. The dataset is collected from the real-world recommendation system of Taobao.\cite{AliCPP}. The dataset contains 4 types of features: user features, item features, combination features and context features, each feature is represented by a unique ID, and they are described as follows:

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Feature Category} & \textbf{Feature ID} & \textbf{Feature Description} \\
        \hline
        \multirow{13}{*}{User Features} & 101 & User ID \\
        \cline{2-3}
        & 109\_14 & User historical behaviors of category ID and count \\
        \cline{2-3}
        & 110\_14 & User historical behaviors of shop ID and count \\
        \cline{2-3}
        & 127\_14 & User historical behaviors of brand ID and count \\
        \cline{2-3}
        & 150\_14 & User historical behaviors of intention node ID and count \\
        \cline{2-3}
        & 121 & Categorical ID of User Profile \\
        \cline{2-3}
        & 122 & Categorical group ID of User Profile \\
        \cline{2-3}
        & 124 & Users Gender ID \\
        \cline{2-3}
        & 125 & Users Age ID \\
        \cline{2-3}
        & 126 & Users Consumption Level Type I \\
        \cline{2-3}
        & 127 & Users Consumption Level Type II \\
        \cline{2-3}
        & 128 & Users Geography Informations \\
        \cline{2-3}
        & 129 & Users Geography Informations \\
        \hline
        \multirow{5}{*}{Item Features} & 205 & Item ID \\
        \cline{2-3}
        & 206 & Category ID to which the item belongs to \\
        \cline{2-3}
        & 207 & Shop ID to which item belongs to \\
        \cline{2-3}
        & 210 & Intention node ID which the item belongs to \\
        \cline{2-3}
        & 216 & Brand ID of the item \\
        \hline
        \multirow{4}{*}{Combination Features} & 508 & The combination of features with 109\_14 and 206 \\
        \cline{2-3}
        & 509 & The combination of features with 110\_14 and 207 \\
        \cline{2-3}
        & 702 & The combination of features with 127\_14 and 216 \\
        \cline{2-3}
        & 853 & The combination of features with 150\_14 and 210 \\
        \hline
        \multirow{1}{*}{Context Features} & 301 & A categorical expression of position \\
        \hline
    \end{tabular}
    \caption{Features Description}
    \label{tab:features}
\end{table}






\section{Experimental Setup}

In order to evaluate the performance of the proposed solution with real-world data, the environment has to be accelerated with a GPU.

\subsection{Hardware Environment}

To achieve this, the experiment is conducted on a cloud-based environment, specifically on an AWS EC2 p3.2xlarge instance \cite{AwsEc2P3}.

The instance is equipped with 8 vCPUs, 61 GiB of memory, and a single NVIDIA Tesla V100 GPU.

The V100 is a high-end GPU that is designed for AI workloads especially deep learning tasks.
It is based on the Volta architecture and is equipped with 5120 CUDA cores, 640 Tensor cores, and 32GB of HBM2 memory with 1.1 TB/s.
It can deliver 7 TFLOPS of double-precision floating-point performance and 116 TFLOPS of deep learning performance.

Figure \ref{fig:V100vsCPU} shows the performance comparison between the Tesla V100 and a CPU.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/v100-vs-cpu.png}
    \caption[Tesla V100 vs CPU]{Tesla V100 vs CPU \cite{NvidiaV100DataSheet}}
    \label{fig:V100vsCPU}
\end{figure}

\subsection{Software Environment}

In addition to the hardware environment, the software environment is also important to be considered.
Having access to \textbf{Nvidia Inception Program} \cite{NvidiaStartups}, 
which includes access to NVIDIA AI Enterprise \cite{NvidiaAiEnterprise} with NGC \cite{NvidiaNGC}.

Instead of setting up the drivers, tools, libraries and frameworks manually, 
the Nvidia AI Enterprise provides a pre-configured flavor of Linux Ubuntu that includes all necessities for deep learning tasks.

In addition to that, instead of compiling the Merlin TensorFlow from the source code, 
the Nvidia Merlin TensorFlow Container \cite{NvidiaMerlinTf} was used as a runtime for the experiment Jupyter notebook.


\section{Evaluation Results}
the solution is conducted using the AliCCP dataset with various models such as MLP, DLRM, DCN, Wide\&Deep, and the Two-Tower model. The evaluation metric used is the AUC. Furthermore, we studied the impact of the number of epochs on the training and validation loss and AUC, using BCE as a loss function.
\subsection{AUC Comparison}

AUC metric describes a model's ability to separate positive and negative instances. A higher AUC indicates stronger distinction between the two classes. Where the AUC value ranges from 0 to 1, where 0.5 is the random guess and 1 is the perfect model.

The AUC metric is used to evaluate the performance of the models on the AliCCP dataset. The results are shown in Figure \ref{fig:AUCComparison}.
% TODO: Remove this
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{assets/models_comparasion.png}
    \caption[AUC Comparison]{AUC Comparison}
    \label{fig:AUCComparison}
\end{figure}

The results show that all the models had a similar performance with AUC values around 0.5, which indicates that the models performed similarly.

Recommending relevant items from millions of possibilities is a significant challenge. 
Although an AUC score of 0.5 might seem underwhelming, 
considering it translates to a 50\% chance of the user clicking on a recommended item becomes more impressive. 
Since systems typically show dozens of items, even a single click signifies the success of the system.

\subsection{Training and Validation}

To study the effect of the number of epochs on the training and validation loss and AUC, we trained the models using BCE as a loss function and plotted the AUC, BCE loss for both training and validation datasets over the number of epochs.
The results are shown in Figure \ref{fig:LossComparison} and Figure \ref{fig:AUCOverEpochs}.

The results show that the training loss kept decreasing over the number of epochs, while the validation loss kept increasing, which indicates that the model is overfitting the training data.


\begin{figure}[H]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=0.95\linewidth]{assets/loss_epochs_20.png}
        \caption[Loss Comparison]{Loss Comparison}
        \label{fig:LossComparison}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=0.95\linewidth]{assets/auc_epochs_20.png}
        \caption[AUC Over Epochs]{AUC Over Epochs}
        \label{fig:AUCOverEpochs}
    \end{subfigure}
    \caption[Measures Over Epoches]{Measures Over Epoches}
\end{figure}
